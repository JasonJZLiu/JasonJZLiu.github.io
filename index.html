<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jason Jingzhou Liu</title>

  <meta name="author" content="Jason Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <!-- <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon"> -->
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <script type="text/javascript" src="js/hidebib.js"></script>

  <script>
    function insertContactLinks() {
      var content = `<p style="text-align:center">
                         <a href="mailto:liujason@cmu.edu">Email</a> &nbsp;|&nbsp;
                         <a href="data/main_page/Jason_Liu_CV_2025-08-26.pdf">CV</a> &nbsp;|&nbsp;
                         <a href="https://scholar.google.com/citations?user=iktexc8AAAAJ">Google Scholar</a> &nbsp;|&nbsp;
                         <a href="https://www.linkedin.com/in/jasonjzliu/">Linkedin</a> &nbsp;|&nbsp;
                         <a href="https://twitter.com/JasonJZLiu">Twitter</a>
                       </p>
                       `;
      document.getElementById('contact-links-desktop').innerHTML = content;
      document.getElementById('contact-links-mobile').innerHTML = content;
    }

    // Call the function when the window loads
    window.onload = insertContactLinks;
    document.addEventListener('DOMContentLoaded', insertContactLinks);
  </script>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <p class="name" style="text-align: center;">
            Jason Jingzhou Liu
          </p>

          <p style="text-align: center;">
            <b>Email:</b> liujason [at] cmu [dot] edu
          </p>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td class="bio">
                  <p style="text-align: justify;">
                    I am a first-year Robotics PhD student at <a href="https://www.ri.cmu.edu/">Carnegie Mellon
                      University</a> in the School of Computer Science. I am advised by
                    <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a> and <a
                      href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>, working on robot learning and
                    dexterous
                    manipulation.
                  </p>

                  <p style="text-align: justify;">
                    I graduated from <a
                      href="https://discover.engineering.utoronto.ca/programs/engineering-programs/engineering-science/">Engineering
                      Science</a>
                    at the University of Toronto, where I received my B.ASc. Previously, I worked with <a
                      href="https://www.cs.toronto.edu/~florian/">Florian Shkurti</a>
                    at the <a href="https://rvl.cs.toronto.edu/">Robot Vision & Learning Lab</a>. I also worked on
                    simulation and robot learning research at <a href="https://developer.nvidia.com/isaac">NVIDIA</a>,
                    collaborating with
                    <a href="https://scholar.google.com/citations?user=sCTJI-0AAAAJ">Ankur Handa</a>,
                    <a href="https://scholar.google.com/citations?user=TCYAoF8AAAAJ">Karl Van Wyk</a>,
                    and <a href="https://www.nathanratliff.com/">Nathan Ratliff</a>.
                  </p>

                  <p style="text-align: justify;">
                    Feel free to contact me via email at liujason [at] cmu [dot] edu!
                  </p>

                  <div id="contact-links-desktop" class="desktoponly"></div>
                </td>
                <td style="padding:2.5%;width:35%;max-width:35%" valign="top">
                  <a href="images/main_page/JasonLiu.jpg">
                    <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 20%;" alt="profile photo"
                      src="images/main_page/JasonLiu.jpg" class="hoverZoomLink">
                  </a>

                  <div id="contact-links-mobile" class="mobileonly"></div>

                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p style="text-align: justify;">
                    I am interested in building robotic systems capable of performing dexterous, robust, human-like
                    skills that can generalize to
                    unstructrued and diverse environemnts. My research focuses on robot learning for manipulation and
                    perception with an emphasis on
                    sim-to-real.
                    Representative papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="drp_stop()" onmouseover="drp_start()" bgcolor="#ffffd0">
                <td class="paperimage">
                  <div class="one" align="center">
                    <div class="two" id='drp_image'>
                      <video width=100% muted autoplay loop>
                        <source src="images/main_page/drp.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/main_page/drp_image.jpg' width=97%>
                  </div>
                  <script type="text/javascript">
                    function drp_start() {
                      document.getElementById('drp_image').style.opacity = "1";
                    }

                    function drp_stop() {
                      document.getElementById('drp_image').style.opacity = "0";
                    }
                    drp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://drp-impact.github.io/">
                    <span class="papertitle">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</span>
                  </a>
                  <br>
                  Jiahui Yang*,
                  <strong>Jason Jingzhou Liu*</strong>,
                  Yulong Li,
                  Youssef Khaky,
                  Kenneth Shaw,
                  Deepak Pathak
                  <br>
                  <br>
                  CoRL 2025 
                  <br>
                  <!-- <p></p> -->
                  <div class="paper" id="drp_tog">
                    <a href="https://drp-impact.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('drp_abs')">abstract</a>
                    <!-- |
                    <a shape="rect" href="javascript:togglebib('drp_tog')" class="togglebib">bibtex</a> -->
                    <!-- | -->
                    <!-- <a href="https://arxiv.org/abs/2505.07813">arXiv</a> -->
                    <!-- |
                    <a href="https://github.com/RaindragonD/factr/">code</a> -->

                    <p align="justify">
                      <i style="display: none;" id="drp_abs">
                        Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge 
                        for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require 
                        full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a 
                        promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize 
                        in complex or dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed 
                        for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. 
                        At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories
                         across diverse simulation scenarios. We further improve IMPACT's static obstacle avoidance through iterative 
                         student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time 
                         using DCP-RMP, a locally reactive goal-proposal module. We evaluate DRP on challenging tasks featuring cluttered 
                         scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior 
                         classical and neural methods in success rate across both simulated and real-world settings. We will release the 
                         dataset, simulation environments, and trained models upon acceptance.
                      </i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{tao2025dexwild,
title={DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies},
author={Tao, Tony and Srirama, Mohan Kumar and Liu, Jason Jingzhou and Shaw, Kenneth and Pathak, Deepak},
journal={Robotics: Science and Systems (RSS)},
year={2025},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>


              <tr onmouseout="dexwild_stop()" onmouseover="dexwild_start()" bgcolor="#ffffd0">
                <td class="paperimage">
                  <div class="one" align="center">
                    <div class="two" id='dexwild_image'>
                      <video width=100% muted autoplay loop>
                        <source src="images/main_page/dexwild.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/main_page/dexwild_image.jpg' width=97%>
                  </div>
                  <script type="text/javascript">
                    function dexwild_start() {
                      document.getElementById('dexwild_image').style.opacity = "1";
                    }

                    function dexwild_stop() {
                      document.getElementById('dexwild_image').style.opacity = "0";
                    }
                    dexwild_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://dexwild.github.io/">
                    <span class="papertitle">DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies</span>
                  </a>
                  <br>
                  Tony Tao*,
                  Mohan Kumar Srirama*,
                  <strong>Jason Jingzhou Liu</strong>,
                  Kenneth Shaw,
                  Deepak Pathak
                  <br>
                  <br>
                  RSS 2025 
                  <br>
                  <b><span style="color: red;">Best Paper Award at EgoAct Workshop</span></b>
                  <br>
                  <!-- <p></p> -->
                  <div class="paper" id="dexwild_tog">
                    <a href="https://dexwild.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('dexwild_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('dexwild_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2505.07813">arXiv</a>
                    |
                    <a href="https://github.com/dexwild/dexwild">code</a>

                    <p align="justify">
                      <i style="display: none;" id="dexwild_abs">
                        Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous
                        manipulation policies
                        to generalize to novel environments, but acquiring such datasets presents many challenges. While
                        teleoperation provides
                        high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use
                        their own hands, just
                        as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses
                        their hands to collect
                        hours of interactions across a multitude of environments and objects. To record this data, we
                        create DexWild-System,
                        a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both
                        human and robot
                        demonstrations, leading to improved performance compared to training on each dataset
                        individually. This combination
                        results in robust robot policies capable of generalizing to novel environments, tasks, and
                        embodiments with minimal
                        additional robot-specific data. Experimental results demonstrate that DexWild significantly
                        improves performance,
                        achieving a 68.5% success rate in unseen environments-nearly four times higher than policies
                        trained with robot data
                        only-and offering 5.8x better cross-embodiment generalization.
                      </i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{tao2025dexwild,
title={DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies},
author={Tao, Tony and Srirama, Mohan Kumar and Liu, Jason Jingzhou and Shaw, Kenneth and Pathak, Deepak},
journal={Robotics: Science and Systems (RSS)},
year={2025},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>


              <tr onmouseout="factr_stop()" onmouseover="factr_start()" bgcolor="#ffffd0">
                <!-- bgcolor="#ffffd0" -->
                <td class="paperimage">
                  <div class="one" align="center">
                    <div class="two" id='factr_image'>
                      <video width=100% muted autoplay loop>
                        <source src="images/main_page/factr.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/main_page/factr_image.jpg' width=97%>
                  </div>
                  <script type="text/javascript">
                    function factr_start() {
                      document.getElementById('factr_image').style.opacity = "1";
                    }

                    function factr_stop() {
                      document.getElementById('factr_image').style.opacity = "0";
                    }
                    factr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://jasonjzliu.com/factr/">
                    <span class="papertitle">FACTR: Force-Attending Curriculum Training
                      for Contact-Rich Policy Learning</span>
                  </a>
                  <br>
                  <strong>Jason Jingzhou Liu*</strong>,
                  Yulong Li*,
                  Kenneth Shaw,
                  Tony Tao,
                  Ruslan Salakhutdinov,
                  Deepak Pathak
                  <br>
                  <br>
                  RSS 2025
                  <br>
                  <div class="paper" id="factr_tog">
                    <a href="https://jasonjzliu.com/factr/">website</a>
                    |
                    <a href="javascript:toggleblock('factr_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('factr_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2502.17432">arXiv</a>
                    |
                    <a href="https://github.com/RaindragonD/factr/">code</a>

                    <p align="justify">
                      <i style="display: none;" id="factr_abs">
                        Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on
                        force feedback for reliable execution. However, this force information, which is readily
                        available in most robot arms, is not commonly used in teleoperation and policy learning.
                        Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not
                        require intricate force-feedback. In this paper, we first present a low-cost, intuitive,
                        bilateral teleoperation setup that relays external forces of the follower arm back to
                        the teacher arm, facilitating data collection for complex, contact-rich tasks. We
                        then introduce FACTR, a policy learning method that employs a curriculum which corrupts
                        the visual input with decreasing intensity throughout training. The curriculum prevents
                        our transformer-based policy from over-fitting to the visual input and guides the policy
                        to properly attend to the force modality. We demonstrate that by fully utilizing the
                        force information, our method significantly improves generalization to unseen objects
                        by 43% compared to baseline approaches without a curriculum.
                      </i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{liu2025factr,
title={FACTR: Force-Attending Curriculum Training for 
Contact-Rich Policy Learning}, 
author={Jason Jingzhou Liu and Yulong Li and Kenneth Shaw 
and Tony Tao and Ruslan Salakhutdinov and Deepak Pathak},
journal={arXiv preprint arXiv:2502.17432},
year={2025},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>



              <tr onmouseout="synthetica_stop()" onmouseover="synthetica_start()">
                <!-- bgcolor="#ffffd0" -->
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='synthetica_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/synthetica_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/synthetica.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function synthetica_start() {
                      document.getElementById('synthetica_image').style.opacity = "1";
                    }

                    function synthetica_stop() {
                      document.getElementById('synthetica_image').style.opacity = "0";
                    }
                    synthetica_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://sites.google.com/view/synthetica-vision">
                    <span class="papertitle">Synthetica: Large Scale Synthetic Data Generation for Robot
                      Perception</span>
                  </a>
                  <br>
                  Ritvik Singh,
                  <strong>Jingzhou Liu</strong>,
                  Karl Van Wyk,
                  Yu-Wei Chao,
                  Jean-Francois Lafleche,
                  Florian Shkurti,
                  Nathan Ratliff,
                  Ankur Handa
                  <br><br>
                  IROS 2025 (Oral)
                  <br>
                  <div class="paper" id="synthetica_tog">
                    <a href="https://sites.google.com/view/synthetica-vision">website</a>
                    |
                    <a href="javascript:toggleblock('synthetica_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('synthetica_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2410.21153">arXiv</a>
                    <!-- | -->
                    <!-- <a href=" TODO ">code</a> -->

                    <p align="justify">
                      <i style="display: none;" id="synthetica_abs">Vision-based object detectors are a crucial basis
                        for robotics
                        applications as they provide valuable information about object localisa-
                        tion in the environment. These need to ensure high reliability in differ-
                        ent lighting conditions, occlusions, and visual artifacts, all while running
                        in real-time. Collecting and annotating real-world data for these net-
                        works is prohibitively time consuming and costly, especially for custom
                        assets, such as industrial objects, making it untenable for generalization
                        to in-the-wild scenarios. To this end, we present Synthetica, a method for
                        large-scale synthetic data generation for training robust state estimators.
                        This paper focuses on the task of object detection, an important problem
                        which can serve as the front-end for most state estimation problems, such
                        as pose estimation. Leveraging data from a photorealistic ray-tracing ren-
                        derer, we scale up data generation, generating 2.7 million images, to train
                        highly accurate real-time detection transformers. We present a collection
                        of rendering randomization and training-time data augmentation tech-
                        niques conducive to robust sim-to-real performance for vision tasks. We
                        demonstrate state-of-the-art performance on the task of object detec-
                        tion while having detectors that run at 50–100Hz which is 9 times faster
                        than the prior SOTA. We further demonstrate the usefulness of our
                        training methodology for robotics applications by showcasing a pipeline
                        for use in the real world with custom objects for which there do not
                        exist prior datasets. Our work highlights the importance of scaling syn-
                        thetic data generation for robust sim-to-real transfer while achieving the
                        fastest real-time inference speeds.</i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{singh2024syntheticalargescalesynthetic,
title={Synthetica: Large Scale Synthetic Data for 
Robot Perception}, 
author={Ritvik Singh and Jingzhou Liu and Karl Van Wyk 
and Yu-Wei Chao and Jean-Francois Lafleche and 
Florian Shkurti and Nathan Ratliff and Ankur Handa},
journal={arXiv},
year={2024},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>


              <tr onmouseout="orbit_surgical_stop()" onmouseover="orbit_surgical_start()">
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='orbit_surgical_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/orbit_surgical_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/orbit_surgical.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function orbit_surgical_start() {
                      document.getElementById('orbit_surgical_image').style.opacity = "1";
                    }

                    function orbit_surgical_stop() {
                      document.getElementById('orbit_surgical_image').style.opacity = "0";
                    }
                    orbit_surgical_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://orbit-surgical.github.io/">
                    <span class="papertitle">ORBIT-Surgical: An Open-Simulation Framework for Accelerated Learning
                      Environments in Surgical Autonomy</span>
                  </a>
                  <br>
                  Qinxi Yu*,
                  Masoud Moghani*,
                  Karthik Dharmarajan,
                  Vincent Schorp,
                  William Chung-Ho Panitch,
                  <strong>Jingzhou Liu</strong>,
                  Kush Hari,
                  Huang Huang,
                  Mayank Mittal,
                  Ken Goldberg,
                  Animesh Garg
                  <br><br>
                  ICRA 2024
                  <br>
                  <div class="paper" id="orbit_surgical_tog">
                    <a href="https://orbit-surgical.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('orbit_surgical_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('orbit_surgical_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2404.16027">arXiv</a>
                    <!-- | -->
                    <!-- <a href=" TODO ">code</a> -->

                    <p align="justify">
                      <i style="display: none;" id="orbit_surgical_abs"> Physics-based simulations have accelerated
                        progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and
                        robust surgical simulation environment remains a challenge. In this paper, we present
                        ORBIT-Surgical, a physics-based surgical robot simulation framework with photorealistic
                        rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research
                        Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical
                        training. ORBIT-Surgical leverages GPU parallelization to train reinforcement learning and
                        imitation learning algorithms to facilitate study of robot learning to augment human surgical
                        skills. ORBIT-Surgical also facilitates realistic synthetic data generation for active
                        perception tasks. We demonstrate ORBIT-Surgical sim-to-real transfer of learned policies onto a
                        physical dVRK robot. </i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{yu2024orbitsurgical,
title={ORBIT-Surgical: An Open-Simulation Framework 
for Learning Surgical Augmented Dexterity}, 
author={Qinxi Yu and Masoud Moghani and 
Karthik Dharmarajan and Vincent Schorp and 
William Chung-Ho Panitch and Jingzhou Liu and 
Kush Hari and Huang Huang and Mayank Mittal and 
Ken Goldberg and Animesh Garg},
journal={arXiv},
year={2024},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>



              <tr onmouseout="handypriors_stop()" onmouseover="handypriors_start()">
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='handypriors_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/handypriors_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/handypriors.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function handypriors_start() {
                      document.getElementById('handypriors_image').style.opacity = "1";
                    }

                    function handypriors_stop() {
                      document.getElementById('handypriors_image').style.opacity = "0";
                    }
                    handypriors_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://handypriors.github.io/">
                    <span class="papertitle">HandyPriors: Physically Consistent Perception of Hand-Object Interactions
                      with Differentiable Priors</span>
                  </a>
                  <br>
                  Shutong Zhang*,
                  Yiling Qiao*,
                  Guanglei Zhu*,
                  Eric Heiden,
                  Dylan Turpin,
                  <strong>Jingzhou Liu</strong>,
                  Ming Lin,
                  Miles Macklin,
                  Animesh Garg
                  <br><br>
                  ICRA 2024
                  <br>
                  <div class="paper" id="handypriors_tog">
                    <a href="https://handypriors.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('handypriors_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('handypriors_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2311.16552">arXiv</a>
                    <!-- |
          <a href=" TODO ">code</a> -->

                    <p align="justify">
                      <i style="display: none;" id="handypriors_abs">Various heuristic objectives for modeling
                        handobject interaction have been proposed in past work. However,
                        due to the lack of a cohesive framework, these objectives often
                        possess a narrow scope of applicability and are limited by their
                        efficiency or accuracy. In this paper, we propose HANDYPRIORS, a unified and general pipeline
                        for pose estimation in
                        human-object interaction scenes by leveraging recent advances
                        in differentiable physics and rendering. Our approach employs
                        rendering priors to align with input images and segmentation masks along with physics priors to
                        mitigate penetration
                        and relative-sliding across frames. Furthermore, we present
                        two alternatives for hand and object pose estimation. The
                        optimization-based pose estimation achieves higher accuracy,
                        while the filtering-based tracking, which utilizes the differentiable priors as dynamics and
                        observation models, executes
                        faster. We demonstrate that HANDYPRIORS attains comparable
                        or superior results in the pose estimation task, and that the
                        differentiable physics module can predict contact information
                        for pose refinement. We also show that our approach generalizes
                        to perception tasks, including robotic hand manipulation and
                        human-object pose estimation in the wild.</i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@InProceedings{zhang2023handypriors,
title={HandyPriors: Physically Consistent Perception 
of Hand-Object Interactions with Differentiable Priors},
author={Shutong Zhang and Yiling Qiao and Guanglei Zhu and 
Eric Heiden and Dylan Turpin and Jingzhou Liu and 
Ming Lin and Miles Macklin and Animesh Garg},
journal={arXiv},
year={2023},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>



              <tr onmouseout="dextreme_stop()" onmouseover="dextreme_start()" bgcolor="#ffffd0">
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='dextreme_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/dextreme_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/dextreme.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function dextreme_start() {
                      document.getElementById('dextreme_image').style.opacity = "1";
                    }

                    function dextreme_stop() {
                      document.getElementById('dextreme_image').style.opacity = "0";
                    }
                    dextreme_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://dextreme.org/">
                    <span class="papertitle">DeXtreme: Transfer of Agile In-Hand Manipulation from Simulation to
                      Reality</span>
                  </a>
                  <br>
                  Ankur Handa*,
                  Arthur Allshire*,
                  Viktor Makoviychuk*,
                  Aleksei Petrenko*,
                  Ritvik Singh*,
                  <strong>Jingzhou Liu*</strong>,
                  Denys Makoviichuk,
                  Karl Van Wyk,
                  Alexander Zhurkevich,
                  Balakumar Sundaralingam,
                  Yashraj Narang,
                  Jean-Francois Lafleche,
                  Dieter Fox,
                  Gavriel State
                  <br><br>
                  ICRA 2023
                  <br>
                  <div class="paper" id="dextreme_tog">
                    <a href="https://dextreme.org/">website</a>
                    |
                    <a href="javascript:toggleblock('dextreme_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('dextreme_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2210.13702">arXiv</a>
                    |
                    <a
                      href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs/tree/main/isaacgymenvs/tasks/dextreme">code</a>

                    <p align="justify">
                      <i style="display: none;" id="dextreme_abs">Recent work has demonstrated the ability of deep
                        reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation,
                        including in the domain of multi-fingered manipulation. However, such models can be challenging
                        to transfer to the real world due to the gap between simulation and reality. In this paper, we
                        present our techniques to train a) a policy that can perform robust dexterous manipulation on an
                        anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable
                        real-time information on the state of the object being manipulated. Our policies are trained to
                        adapt to a wide range of conditions in simulation. Consequently, our vision-based policies
                        significantly outperform the best vision policies in the literature on the same reorientation
                        task and are competitive with policies that are given privileged state information via motion
                        capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous
                        manipulation in diverse kinds of hardware and simulator setups, and in our case, with the
                        Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for
                        researchers to achieve such results with commonly-available, affordable robot hands and
                        cameras.</i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{nvidia2022dextreme,
title={DeXtreme: Transfer of Agile In-Hand 
Manipulation from Simulation to Reality},
author={Handa, Ankur and Allshire, Arthur and 
Makoviychuk, Viktor and Petrenko, Aleksei and 
Singh, Ritvik and Liu, Jingzhou and 
Makoviychuk, Denys and Van Wyk, Karl and 
Zhurkevich, Alexander and Sundaralingam, Balakumar
and Narang, Yashraj and Lafleche, Jean-Francois and 
Fox, Dieter and State, Gavriel},
journal={arXiv},
year={2022},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>



              <tr onmouseout="fast_graspd_stop()" onmouseover="fast_graspd_start()">
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='fast_graspd_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/fast_graspd_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/fast_graspd.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function fast_graspd_start() {
                      document.getElementById('fast_graspd_image').style.opacity = "1";
                    }

                    function fast_graspd_stop() {
                      document.getElementById('fast_graspd_image').style.opacity = "0";
                    }
                    fast_graspd_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://dexgrasp.github.io/">
                    <span class="papertitle">Fast-Grasp'D: Dexterous Multi-finger Grasp Generation Through
                      Differentiable Simulation</span>
                  </a>
                  <br>
                  Dylan Turpin,
                  Tao Zhong,
                  Shutong Zhang,
                  Guanglei Zhu,
                  <strong>Jingzhou Liu</strong>,
                  Ritvik Singh,
                  Eric Heiden,
                  Miles Macklin,
                  Stavros Tsogkas,
                  Sven Dickinson,
                  Animesh Garg
                  <br><br>
                  ICRA 2023
                  <br>
                  <div class="paper" id="fast_graspd_tog">
                    <a href="https://dexgrasp.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('fast_graspd_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('fast_graspd_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2306.08132">arXiv</a>
                    <!-- |
          <a href=" TODO ">code</a> -->

                    <p align="justify">
                      <i style="display: none;" id="fast_graspd_abs"> Multi-finger grasping relies on high quality
                        training data, which is hard to obtain: human data is hard to transfer and synthetic data relies
                        on simplifying assumptions that reduce grasp quality. By making grasp simulation differentiable,
                        and contact dynamics amenable to gradient-based optimization, we accelerate the search for
                        high-quality grasps with fewer limiting assumptions. We present Grasp’D-1M: a large-scale
                        dataset for multi-finger robotic grasping, synthesized with Fast- Grasp’D, a novel
                        differentiable grasping simulator. Grasp’D- 1M contains one million training examples for three
                        robotic hands (three, four and five-fingered), each with multimodal visual inputs
                        (RGB+depth+segmentation, available in mono and stereo). Grasp synthesis with Fast-Grasp’D is 10x
                        faster than GraspIt! and 20x faster than the prior Grasp’D differentiable simulator. Generated
                        grasps are more stable and contact-rich than GraspIt! grasps, regardless of the distance
                        threshold used for contact generation. We validate the usefulness of our dataset by retraining
                        an existing vision-based grasping pipeline on Grasp’D-1M, and showing a dramatic increase in
                        model performance, predicting grasps with 30% more contact, a 33% higher epsilon metric, and 35%
                        lower simulated displacement. </i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{turpin2023fastgraspd,
title={Fast-Grasp'D: Dexterous Multi-finger Grasp 
Generation Through Differentiable Simulation}, 
author={Dylan Turpin and Tao Zhong and Shutong Zhang and 
Guanglei Zhu and Jingzhou Liu and Ritvik Singh and 
Eric Heiden and Miles Macklin and Stavros Tsogkas and 
Sven Dickinson and Animesh Garg},
journal={arXiv},
year={2023},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>



              <tr onmouseout="geomatch_stop()" onmouseover="geomatch_start()">
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='geomatch_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/geomatch_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/geomatch.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function geomatch_start() {
                      document.getElementById('geomatch_image').style.opacity = "1";
                    }

                    function geomatch_stop() {
                      document.getElementById('geomatch_image').style.opacity = "0";
                    }
                    geomatch_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://geo-match.github.io/">
                    <span class="papertitle">Geometry Matching for Multi-Embodiment Grasping.</span>
                  </a>
                  <br>
                  Maria Attarian,
                  Muhammad Adil Asif,
                  <strong>Jingzhou Liu</strong>,
                  Ruthrash Hari,
                  Animesh Garg,
                  Igor Gilitschenski,
                  Jonathan Tompson
                  <br><br>
                  CoRL 2023
                  <br>
                  <div class="paper" id="geomatch_tog">
                    <a href="https://geo-match.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('geomatch_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('geomatch_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2312.03864">arXiv</a>
                    |
                    <a href="https://github.com/google-deepmind/geomatch">code</a>

                    <p align="justify">
                      <i style="display: none;" id="geomatch_abs">Many existing learning-based grasping approaches
                        concentrate on a single embodiment, provide limited generalization to higher DoF end-effectors
                        and cannot capture a diverse set of grasp modes. We tackle the problem of grasping using
                        multiple embodiments by learning rich geometric representations for both objects and
                        end-effectors using Graph Neural Networks. Our novel method - GeoMatch - applies supervised
                        learning on grasping data from multiple embodiments, learning end-to-end contact point
                        likelihood maps as well as conditional autoregressive predictions of grasps
                        keypoint-by-keypoint. We compare our method against baselines that support multiple embodiments.
                        Our approach performs better across three end-effectors, while also producing diverse
                        grasps.</i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{attarian2023geometry,
title={Geometry Matching for Multi-Embodiment Grasping}, 
author={Maria Attarian and Muhammad Adil Asif and 
Jingzhou Liu and Ruthrash Hari and Animesh Garg and 
Igor Gilitschenski and Jonathan Tompson},
journal={arXiv},
year={2023},
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>


              <tr onmouseout="orbit_stop()" onmouseover="orbit_start()" bgcolor="#ffffd0">
                <td class="paperimage">
                  <div class="one">
                    <div class="two" id='orbit_image'><video width=100% muted autoplay loop>
                        <source src="images/main_page/orbit_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/main_page/orbit.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function orbit_start() {
                      document.getElementById('orbit_image').style.opacity = "1";
                    }

                    function orbit_stop() {
                      document.getElementById('orbit_image').style.opacity = "0";
                    }
                    orbit_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify;">
                  <a href="https://isaac-orbit.github.io/">
                    <span class="papertitle">Orbit: A Unified Simulation Framework for Interactive Robot Learning
                      Environments</span>
                  </a>
                  <br>
                  Mayank Mittal,
                  Calvin Yu,
                  Qinxi Yu,
                  <strong>Jingzhou Liu</strong>,
                  Nikita Rudin,
                  David Hoeller,
                  Jia Lin Yuan,
                  Ritvik Singh,
                  Yunrong Guo
                  Hammad Mazhar,
                  Ajay Mandlekar,
                  Buck Babich,
                  Gavriel State,
                  Marco Hutter,
                  Animesh Garg
                  <br><br>
                  IROS 2023 | RA-L
                  <br>
                  <div class="paper" id="orbit_tog">
                    <a href="https://isaac-orbit.github.io/">website</a>
                    |
                    <a href="javascript:toggleblock('orbit_abs')">abstract</a>
                    |
                    <a shape="rect" href="javascript:togglebib('orbit_tog')" class="togglebib">bibtex</a>
                    |
                    <a href="https://arxiv.org/abs/2301.04195">arXiv</a>
                    |
                    <a href="https://github.com/NVIDIA-Omniverse/orbit">code</a>

                    <p align="justify">
                      <i style="display: none;" id="orbit_abs">We present Orbit, a unified and modular framework for
                        robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently
                        create robotic environments with photo-realistic scenes and high-fidelity rigid and deformable
                        body simulation. With Orbit, we provide a suite of benchmark tasks of varying difficulty -- from
                        single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization.
                        To support working with diverse observations and action spaces, we include fixed-arm and mobile
                        manipulators with different physically-based sensors and motion generators. Orbit allows
                        training reinforcement learning policies and collecting large demonstration datasets from
                        hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization.
                        In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4
                        sensor modalities, 10 motion generators, more than 20 benchmark tasks, and wrappers to 4
                        learning libraries. With this framework, we aim to support various research areas, including
                        representation learning, reinforcement learning, imitation learning, and task and motion
                        planning. We hope it helps establish interdisciplinary collaborations in these communities, and
                        its modularity makes it easily extensible for more tasks and applications in the future.</i>
                    </p>

                    <pre xml:space="preserve" style="display:none;">
@article{mittal2023orbit,
title={Orbit: A Unified Simulation Framework for 
Interactive Robot Learning Environments}, 
author={Mittal, Mayank and Yu, Calvin and 
Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and 
Hoeller, David and Yuan, Jia Lin and 
Singh, Ritvik and Guo, Yunrong and 
Mazhar, Hammad and Mandlekar, Ajay and 
Babich, Buck and State, Gavriel and 
Hutter, Marco and Garg, Animesh},
journal={IEEE Robotics and Automation Letters}, 
year={2023},
volume={8},
number={6},
pages={3740-3747},
doi={10.1109/LRA.2023.3270034}
}
          </pre>
                  </div>
                  <p></p>
                  <p></p>
                </td>
              </tr>


            </tbody>
          </table>





          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding-top: 40px;">
                  <h2>Education</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="0">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
                  <a href="https://www.ri.cmu.edu/">
                    <img src="images/main_page/cmu.png" style="max-width:70%;">
                  </a>
                </td>
                <td width="50%" valign="center">
                  <b>Ph.D in Robotics</b>
                  <br>
                  School of Computer Science
                  <br><br>
                  Carnegie Mellon University
                </td>
                <td width="25%" valign="center" align="right">
                  2024 - Present
                </td>
              </tr>
              <tr style="height:20px;" class="mobileonly">
                <td colspan="3"></td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align:center;">
                  <a href="https://discover.engineering.utoronto.ca/programs/engineering-programs/engineering-science/">
                    <img src="images/main_page/uoft.png" style="max-width:90%;">
                  </a>
                </td>
                <td width="50%" valign="center">
                  <b>Bachelor of Applied Science and Engineering</b>
                  <br>
                  Engineering Science Robotics
                  <br><br>
                  University of Toronto
                  <br>
                </td>
                <td width="25%" valign="center" align="right">
                  2019 - 2024
                </td>
              </tr>
            </tbody>
          </table>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding-top: 40px;">
                  <h2>Personal Projects</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                  <tr onmouseout="eng_sci_capstone_stop()" onmouseover="eng_sci_capstone_start()">
                    <td class="paperimage">
                      <div class="one">
                        <div class="two" id='eng_sci_capstone_image'><video width=100% muted autoplay loop>
                            <source src="images/main_page/eng_sci_capstone_video.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                          </video></div>
                        <img src='images/main_page/eng_sci_capstone.png' width=100%>
                      </div>
                      <script type="text/javascript">
                        function eng_sci_capstone_start() {
                          document.getElementById('eng_sci_capstone_image').style.opacity = "1";
                        }

                        function eng_sci_capstone_stop() {
                          document.getElementById('eng_sci_capstone_image').style.opacity = "0";
                        }
                        eng_sci_capstone_stop()
                      </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://github.com/JasonJZLiu/ROB498-Capstone-Project">
                        <span class="papertitle">Safe Drone Teleoperation with Real Time Mapping and Obstacle
                          Avoidance</span>
                      </a>
                      <br>
                      Engineering Science Robotics Capstone
                      <br>
                      University of Toronto
                      <br>
                      <p></p>
                      <div class="paper" id="eng_sci_capstone_tog">
                        <a href="https://github.com/JasonJZLiu/ROB498-Capstone-Project">website</a>
                        |
                        <a href="javascript:toggleblock('eng_sci_capstone_abs')">abstract</a>
                        |
                        <a
                          href="https://youtube.com/playlist?list=PLXA2xQ2PoSDIsXgDwbFxVmVYZtm3Uqi-j&si=yD0VoFYAUGvRks8F">video</a>
                        |
                        <a href="https://github.com/JasonJZLiu/ROB498-Capstone-Project">code</a>

                        <p align="justify">
                          <i style="display: none;" id="eng_sci_capstone_abs">
                            Teleoperating a drone in a safe manner can be challenging, particularly in cluttered indoor
                            environments with an abundance of obstacles. We present a safe teleoperation system for
                            drones by performing automatic real-time dynamic obstacle avoidance, allowing us to expose a
                            suite of simplified high-level control primitives to the drone operator such as "fly
                            forward", "fly to the left", "fly up", "rotate", etc. This system reduces the complexity and
                            the extent of the manual controls required from drone operators to fly the drone safely. The
                            system accomplishes this by constructing a dynamic map of its environment in real-time and
                            continuously performing path-planning using the map in order to execute a collision-free
                            path to the desired user-specified position target.
                          </i>
                        </p>
                      </div>
                      <p></p>
                      <p></p>
                    </td>
                  </tr>


                  <!-- <tr onmouseout="ball_balance_stop()" onmouseover="ball_balance_start()">
            <td class="paperimage">
              <div class="one">
                <div class="two" id='ball_balance_image'><video  width=100% muted autoplay loop>
                <source src="images/main_page/ball_balance_video.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/main_page/ball_balance.png' width=100%>
              </div>
              <script type="text/javascript">
                function ball_balance_start() {
                  document.getElementById('ball_balance_image').style.opacity = "1";
                }
      
                function ball_balance_stop() {
                  document.getElementById('ball_balance_image').style.opacity = "0";
                }
                ball_balance_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/JasonJZLiu/BallBalancingTable">
                <span class="papertitle">Vision-Based Ball Balancing Table</span>
              </a>
              <br>
              MIE438 Course Project
              <br>
              University of Toronto
              <br>
              <p></p>
              <div class="paper" id="eng_sci_capstone_tog">
                <a href="https://github.com/JasonJZLiu/BallBalancingTable">website</a>
                |
                <a href="javascript:toggleblock('ball_balance_abs')">abstract</a>
                |
                <a href="https://www.youtube.com/watch?v=9Cm7HZLUdoY">video</a>
                |
                <a href="https://github.com/JasonJZLiu/BallBalancingTable">code</a>
      
                <p align="justify">
                  <i style="display: none;" id="ball_balance_abs">
                    The ball-balancing table is a robotic platform capable of controlling the position of a ball on a plate by adjusting the plate’s orientation via feedback control. The system has two degrees-of-freedom (DOF), actuated by two servos. The ball position is obtained in real-time by capturing bird’s eye view images using a camera and performing image processing to track the pixel coordinates of the center of the ball. For each servo, the output commands are computed by calculating the error signal and passing it through a PID controller to regulate the ball position.
                  </i>
                </p>
              </div>
              <p></p>
              <p></p>
            </td>
          </tr> -->


                </tbody>
              </table>







              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:xx-small;">
                        Modified template from <a style="font-size:xx-small;"
                          href="https://github.com/jonbarron/jonbarron_website">here</a>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>